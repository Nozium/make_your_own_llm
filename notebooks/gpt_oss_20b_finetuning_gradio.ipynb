{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ GPT-OSS 20B ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (Gradio UIç‰ˆ)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Gradio UIã‚’ä½¿ã£ã¦ç›´æ„Ÿçš„ã«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "## ä½¿ã„æ–¹\n",
    "1. ã€Œ**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **ã€â†’ã€Œ**ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ**ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. æœ€å¾Œã®ã‚»ãƒ«ã«UIãŒè¡¨ç¤ºã•ã‚Œã¾ã™\n",
    "3. UIã®å„ã‚¿ãƒ–ã§æ“ä½œã—ã¦ãã ã•ã„\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ [Unsloth](https://github.com/unslothai/unsloth) ã®\n",
    "[LGPL-3.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme) ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å…ƒã«ä½œæˆã—ã¾ã—ãŸã€‚\n",
    "\n",
    "**å¤‰æ›´ç‚¹**: Gradio UIã®è¿½åŠ ã€æ—¥æœ¬èªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ“¦ ã‚»ãƒ«1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "print(\"ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo\n",
    "\n",
    "# Gradioã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q gradio\n",
    "\n",
    "print(\"âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ¤– ã‚»ãƒ«2: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "print(\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    dtype = None,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼\")\n",
    "\n",
    "# GPUæƒ…å ±\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "print(f\"ğŸ–¥ï¸ GPU: {gpu_stats.name}\")\n",
    "print(f\"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {round(gpu_stats.total_memory / 1024 / 1024 / 1024, 1)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ğŸ¨ ã‚»ãƒ«3: Gradio UI\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nimport gradio as gr\nimport json\nimport shutil\nimport os\nfrom datasets import Dataset, load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nstate = {\n    \"dataset\": None,\n    \"trainer\": None,\n    \"model_with_lora\": None,\n    \"training_done\": False,\n    \"logs\": []\n}\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\ndef load_chatgpt_conversations(file_path):\n    \"\"\"ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’å¤‰æ›\"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    training_data = []\n    for conversation in data:\n        messages = []\n        mapping = conversation.get('mapping', {})\n        for node_id, node in mapping.items():\n            msg = node.get('message')\n            if msg and msg.get('content', {}).get('parts'):\n                role = msg['author']['role']\n                parts = msg['content']['parts']\n                if parts and isinstance(parts[0], str):\n                    content = parts[0].strip()\n                    if role in ['user', 'assistant'] and content:\n                        messages.append({\"role\": role, \"content\": content})\n        if len(messages) >= 2:\n            training_data.append({\"messages\": messages})\n    return training_data\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n    return {\"text\": texts}\n\ndef load_data(file, use_sample, min_turns, max_turns):\n    \"\"\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\"\"\"\n    global state\n    try:\n        if use_sample:\n            dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n            dataset = standardize_sharegpt(dataset)\n        else:\n            if file is None:\n                return \"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\"\n            raw_data = load_chatgpt_conversations(file.name)\n            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n            filtered = [c for c in raw_data if min_turns <= len(c['messages']) <= max_turns]\n            dataset = Dataset.from_list(filtered)\n\n        dataset = dataset.map(formatting_prompts_func, batched=True)\n        state[\"dataset\"] = dataset\n        return f\"âœ… {len(dataset)} ä»¶ã®ä¼šè©±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\"\n    except Exception as e:\n        return f\"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# å­¦ç¿’é–¢æ•°\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\ndef start_training(max_steps, learning_rate, batch_size, lora_r, progress=gr.Progress()):\n    \"\"\"å­¦ç¿’å®Ÿè¡Œ\"\"\"\n    global state, model\n\n    if state[\"dataset\"] is None:\n        return \"âŒ å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\", None\n\n    try:\n        progress(0, desc=\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...\")\n\n        # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š\n        model_with_lora = FastLanguageModel.get_peft_model(\n            model,\n            r = int(lora_r),\n            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                              \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_alpha = 16,\n            lora_dropout = 0,\n            bias = \"none\",\n            use_gradient_checkpointing = \"unsloth\",\n            random_state = 3407,\n        )\n\n        progress(0.1, desc=\"ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’è¨­å®šä¸­...\")\n\n        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š\n        trainer = SFTTrainer(\n            model = model_with_lora,\n            tokenizer = tokenizer,\n            train_dataset = state[\"dataset\"],\n            args = SFTConfig(\n                per_device_train_batch_size = int(batch_size),\n                gradient_accumulation_steps = 4,\n                warmup_steps = 5,\n                max_steps = int(max_steps),\n                learning_rate = float(learning_rate),\n                logging_steps = 1,\n                optim = \"adamw_8bit\",\n                weight_decay = 0.001,\n                lr_scheduler_type = \"linear\",\n                seed = 3407,\n                output_dir = \"outputs\",\n                report_to = \"none\",\n            ),\n        )\n\n        # å›ç­”éƒ¨åˆ†ã®ã¿å­¦ç¿’\n        trainer = train_on_responses_only(\n            trainer,\n            instruction_part = \"<|start|>user<|message|>\",\n            response_part = \"<|start|>assistant<|channel|>final<|message|>\"\n        )\n\n        progress(0.2, desc=\"å­¦ç¿’é–‹å§‹...\")\n\n        # å­¦ç¿’å®Ÿè¡Œ\n        trainer_stats = trainer.train()\n\n        state[\"model_with_lora\"] = model_with_lora\n        state[\"training_done\"] = True\n\n        # çµæœ\n        runtime = trainer_stats.metrics['train_runtime']\n        result = f\"\"\"âœ… å­¦ç¿’å®Œäº†ï¼\n\nâ±ï¸ å­¦ç¿’æ™‚é–“: {round(runtime, 1)} ç§’ ({round(runtime/60, 1)} åˆ†)\nğŸ“Š æœ€çµ‚Loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\n\"\"\"\n        return result, None\n\n    except Exception as e:\n        return f\"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\", None\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# æ¨è«–é–¢æ•°\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\ndef test_model(message, reasoning_effort, max_tokens):\n    \"\"\"ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ\"\"\"\n    global state\n\n    if not state[\"training_done\"]:\n        return \"âš ï¸ å…ˆã«å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\"\n\n    try:\n        model_to_use = state[\"model_with_lora\"]\n\n        messages = [{\"role\": \"user\", \"content\": message}]\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt = True,\n            return_tensors = \"pt\",\n            return_dict = True,\n            reasoning_effort = reasoning_effort,\n        ).to(\"cuda\")\n\n        outputs = model_to_use.generate(\n            **inputs,\n            max_new_tokens = int(max_tokens),\n            use_cache = True,\n        )\n\n        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”éƒ¨åˆ†ã‚’æŠ½å‡º\n        if \"<|start|>assistant\" in response:\n            response = response.split(\"<|start|>assistant\")[-1]\n            response = response.replace(\"<|channel|>final<|message|>\", \"\").replace(\"<|end|>\", \"\").replace(\"<|return|>\", \"\").strip()\n\n        return response\n    except Exception as e:\n        return f\"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ä¿å­˜é–¢æ•°\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\ndef save_model(save_location, model_name):\n    \"\"\"ãƒ¢ãƒ‡ãƒ«ä¿å­˜\"\"\"\n    global state\n\n    if not state[\"training_done\"]:\n        return \"âš ï¸ å…ˆã«å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\", None\n\n    try:\n        model_to_save = state[\"model_with_lora\"]\n\n        if save_location == \"colab\":\n            model_to_save.save_pretrained(model_name)\n            return f\"âœ… '{model_name}' ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆColabã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã§æ¶ˆãˆã¾ã™ï¼‰\", None\n\n        elif save_location == \"gdrive\":\n            from google.colab import drive\n            drive.mount('/content/drive')\n            save_path = f\"/content/drive/MyDrive/{model_name}\"\n            model_to_save.save_pretrained(save_path)\n            return f\"âœ… Google Drive ã«ä¿å­˜ã—ã¾ã—ãŸ: {save_path}\", None\n\n        elif save_location == \"huggingface\":\n            return \"ğŸ“ HuggingFaceã¸ã®ä¿å­˜ã¯ã€ä¸‹è¨˜ã‚³ãƒ¼ãƒ‰ã‚’åˆ¥ã‚»ãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:\\n\\nmodel.push_to_hub('your-username/model-name', token='hf_...')\", None\n\n    except Exception as e:\n        return f\"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\", None\n\ndef download_model(model_name):\n    \"\"\"ãƒ¢ãƒ‡ãƒ«ã‚’ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\"\"\"\n    global state\n\n    if not state[\"training_done\"]:\n        return None\n\n    try:\n        # ä¿å­˜\n        model_to_save = state[\"model_with_lora\"]\n        model_to_save.save_pretrained(model_name)\n\n        # ZIPä½œæˆ\n        zip_path = f\"{model_name}.zip\"\n        shutil.make_archive(model_name, 'zip', model_name)\n\n        return zip_path\n    except:\n        return None\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# Gradio UI\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nwith gr.Blocks(title=\"GPT-OSS 20B Fine-tuning\", theme=gr.themes.Soft(), fill_height=True) as demo:\n    gr.Markdown(\"# ğŸ“ GPT-OSS 20B ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\")\n    gr.Markdown(\"UIã§ç›´æ„Ÿçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã§ãã¾ã™\")\n\n    with gr.Tabs():\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        # ã‚¿ãƒ–1: ãƒ‡ãƒ¼ã‚¿æº–å‚™\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        with gr.Tab(\"ğŸ“¤ ãƒ‡ãƒ¼ã‚¿æº–å‚™\"):\n            gr.Markdown(\"### ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ\")\n\n            use_sample = gr.Checkbox(\n                label=\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒãªã„å ´åˆï¼‰\",\n                value=True\n            )\n\n            file_input = gr.File(\n                label=\"conversations.json ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\",\n                file_types=[\".json\"],\n                visible=False\n            )\n\n            with gr.Row():\n                min_turns = gr.Slider(1, 10, value=2, step=1, label=\"æœ€å°ã‚¿ãƒ¼ãƒ³æ•°\")\n                max_turns = gr.Slider(5, 50, value=20, step=5, label=\"æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°\")\n\n            load_btn = gr.Button(\"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\", variant=\"primary\")\n            load_result = gr.Textbox(label=\"çµæœ\", interactive=False)\n\n            # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®å¤‰æ›´ã§ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ã®è¡¨ç¤ºåˆ‡ã‚Šæ›¿ãˆ\n            use_sample.change(\n                fn=lambda x: gr.update(visible=not x),\n                inputs=[use_sample],\n                outputs=[file_input]\n            )\n\n            load_btn.click(\n                fn=load_data,\n                inputs=[file_input, use_sample, min_turns, max_turns],\n                outputs=[load_result]\n            )\n\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        # ã‚¿ãƒ–2: å­¦ç¿’è¨­å®š & å®Ÿè¡Œ\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        with gr.Tab(\"ğŸš€ å­¦ç¿’\"):\n            gr.Markdown(\"### å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")\n\n            with gr.Row():\n                max_steps = gr.Slider(10, 500, value=30, step=10,\n                    label=\"å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°\",\n                    info=\"30: ãƒ†ã‚¹ãƒˆ(10åˆ†) / 100: è»½ã‚(30åˆ†) / 500: ã—ã£ã‹ã‚Š(2æ™‚é–“)\")\n\n            with gr.Row():\n                learning_rate = gr.Number(value=2e-4, label=\"å­¦ç¿’ç‡\", info=\"æ¨å¥¨: 2e-4\")\n                batch_size = gr.Slider(1, 4, value=1, step=1, label=\"ãƒãƒƒãƒã‚µã‚¤ã‚º\")\n\n            with gr.Row():\n                lora_r = gr.Dropdown([8, 16, 32, 64, 128], value=8,\n                    label=\"LoRAãƒ©ãƒ³ã‚¯\", info=\"å¤§ãã„ã»ã©è¡¨ç¾åŠ›â†‘ã€ãƒ¡ãƒ¢ãƒªâ†‘\")\n\n            train_btn = gr.Button(\"ğŸš€ å­¦ç¿’ã‚’é–‹å§‹\", variant=\"primary\", size=\"lg\")\n            train_result = gr.Textbox(label=\"å­¦ç¿’çµæœ\", interactive=False, lines=5)\n            train_plot = gr.Plot(label=\"å­¦ç¿’æ›²ç·š\", visible=False)\n\n            train_btn.click(\n                fn=start_training,\n                inputs=[max_steps, learning_rate, batch_size, lora_r],\n                outputs=[train_result, train_plot]\n            )\n\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        # ã‚¿ãƒ–3: ãƒ†ã‚¹ãƒˆ\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        with gr.Tab(\"ğŸ’¬ ãƒ†ã‚¹ãƒˆ\"):\n            gr.Markdown(\"### å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ\")\n\n            test_input = gr.Textbox(\n                label=\"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›\",\n                placeholder=\"ã“ã‚“ã«ã¡ã¯ï¼è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚\",\n                lines=3\n            )\n\n            with gr.Row():\n                reasoning = gr.Radio(\n                    [\"low\", \"medium\", \"high\"],\n                    value=\"medium\",\n                    label=\"æ¨è«–ãƒ¬ãƒ™ãƒ«\",\n                    info=\"low: é«˜é€Ÿ / medium: ãƒãƒ©ãƒ³ã‚¹ / high: è©³ç´°\"\n                )\n                max_tokens = gr.Slider(64, 512, value=256, step=64, label=\"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\")\n\n            test_btn = gr.Button(\"ğŸ’¬ é€ä¿¡\", variant=\"primary\")\n            test_output = gr.Textbox(label=\"ãƒ¢ãƒ‡ãƒ«ã®å›ç­”\", interactive=False, lines=10)\n\n            test_btn.click(\n                fn=test_model,\n                inputs=[test_input, reasoning, max_tokens],\n                outputs=[test_output]\n            )\n\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        # ã‚¿ãƒ–4: ä¿å­˜\n        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n        with gr.Tab(\"ğŸ’¾ ä¿å­˜\"):\n            gr.Markdown(\"### ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\")\n\n            model_name_input = gr.Textbox(\n                label=\"ãƒ¢ãƒ‡ãƒ«å\",\n                value=\"my_gpt_oss_20b\",\n                placeholder=\"my_gpt_oss_20b\"\n            )\n\n            save_location = gr.Radio(\n                [\"colab\", \"gdrive\", \"huggingface\"],\n                value=\"colab\",\n                label=\"ä¿å­˜å…ˆ\",\n                info=\"colab: ä¸€æ™‚ä¿å­˜ / gdrive: Google Drive / huggingface: HuggingFace Hub\"\n            )\n\n            save_btn = gr.Button(\"ğŸ’¾ ä¿å­˜\", variant=\"primary\")\n            save_result = gr.Textbox(label=\"çµæœ\", interactive=False)\n\n            gr.Markdown(\"---\")\n            gr.Markdown(\"### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n\n            download_btn = gr.Button(\"ğŸ“¥ ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n            download_file = gr.File(label=\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\", interactive=False)\n\n            save_btn.click(\n                fn=save_model,\n                inputs=[save_location, model_name_input],\n                outputs=[save_result, download_file]\n            )\n\n            download_btn.click(\n                fn=download_model,\n                inputs=[model_name_input],\n                outputs=[download_file]\n            )\n\n    gr.Markdown(\"---\")\n    gr.Markdown(\"ğŸ“œ [LGPL-3.0 License](https://github.com/unslothai/notebooks) | Powered by [Unsloth](https://unsloth.ai)\")\n\n# UIã‚’èµ·å‹•ï¼ˆé«˜ã•ã‚’800pxã«è¨­å®šï¼‰\ndemo.launch(share=False, debug=True, height=800)"
  }
 ]
}