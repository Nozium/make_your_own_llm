{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ è‡ªåˆ†ã ã‘ã®GPT-OSS 20Bã‚’ä½œã‚ã†ï¼\n",
    "\n",
    "## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã§ãã‚‹ã“ã¨\n",
    "\n",
    "ã‚ãªãŸã®ChatGPTã¨ã®ä¼šè©±å±¥æ­´ã‚’ä½¿ã£ã¦ã€**è‡ªåˆ†å°‚ç”¨ã®AIãƒ¢ãƒ‡ãƒ«**ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "Google ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æŒã£ã¦ã„ã‚Œã°ã€èª°ã§ã‚‚ç„¡æ–™ã§å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚\n",
    "æœ€åˆã«ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€å·¦ä¸Šã®ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã€â†’ã€Œãƒ‰ãƒ©ã‚¤ãƒ–ã«ã‚³ãƒ”ãƒ¼ã‚’ä¿å­˜ã€ã—ã¦è‡ªåˆ†ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ãã®å¾Œã€ä»¥ä¸‹ã®ã‚ˆã†ãªæµã‚Œã§å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™!\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"ğŸ“¥ ã‚ãªãŸã®<br>ChatGPTå±¥æ­´\"] --> B[\"ğŸ“ å­¦ç¿’<br>(ç´„10åˆ†)\"]\n",
    "    B --> C[\"ğŸ¤– è‡ªåˆ†å°‚ç”¨ã®<br>GPT-OSS 20B\"]\n",
    "    \n",
    "    style A fill:#e1f5fe,stroke:#01579b\n",
    "    style B fill:#fff3e0,stroke:#e65100\n",
    "    style C fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "### ãªãœè‡ªåˆ†å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã®ï¼Ÿ\n",
    "\n",
    "- ğŸ¯ ã‚ãªãŸã®å£èª¿ãƒ»æ–‡ä½“ã‚’å­¦ç¿’\n",
    "- ğŸ“š ã‚ãªãŸãŒã‚ˆãè©±ã™è©±é¡Œã«è©³ã—ããªã‚‹\n",
    "- ğŸ”’ ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã›ã‚‹ï¼ˆAPIæ–™é‡‘ä¸è¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸƒ å§‹ã‚æ–¹\n",
    "\n",
    "ç”»é¢ä¸Šéƒ¨ã®ã€Œ**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **ã€â†’ã€Œ**ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ**ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã ã‘ï¼\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨è¬è¾\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ [Unsloth](https://github.com/unslothai/unsloth) ã®\n",
    "[LGPL-3.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme) ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å…ƒã«ä½œæˆã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### å¤‰æ›´ç‚¹\n",
    "- ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªåŒ–\n",
    "- ğŸ’¬ ChatGPTä¼šè©±å±¥æ­´ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’è¿½åŠ \n",
    "- ğŸ“– åˆå¿ƒè€…å‘ã‘è§£èª¬ã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¬ã‚¤ãƒ‰ã‚’è¿½åŠ \n",
    "- ğŸ”§ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ‹¡å……\n",
    "\n",
    "å…ƒã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb\n",
    "ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: LGPL-3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– çŸ¥ã£ã¦ãŠãã¨ä¾¿åˆ©ãªç”¨èª\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ”§ <b>Fine-tuningï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰</b></summary>\n",
    "\n",
    "ã™ã§ã«å­¦ç¿’æ¸ˆã¿ã®AIãƒ¢ãƒ‡ãƒ«ã«ã€è¿½åŠ ã§è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã€‚  \n",
    "ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹ã‚ˆã‚Šåœ§å€’çš„ã«å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ãƒ»æ™‚é–“ã§æ¸ˆã¿ã¾ã™ã€‚\n",
    "\n",
    "ä¾‹ãˆã‚‹ãªã‚‰ï¼šè‹±èªãŒè©±ã›ã‚‹äººã«æ—¥æœ¬èªã‚’æ•™ãˆã‚‹ã‚ˆã†ãªã‚‚ã®  \n",
    "ï¼ˆèµ¤ã¡ã‚ƒã‚“ã«ã‚¼ãƒ­ã‹ã‚‰è¨€èªã‚’æ•™ãˆã‚‹ã‚ˆã‚Šæ—©ã„ï¼ï¼‰\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ§© <b>LoRAï¼ˆãƒ­ãƒ¼ãƒ©ï¼‰</b></summary>\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã§ã¯ãªãã€ä¸€éƒ¨ã ã‘ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’ã™ã‚‹æŠ€è¡“ã€‚  \n",
    "ãƒ¡ãƒ¢ãƒªãŒå°‘ãªãã¦ã‚‚å­¦ç¿’ã§ãã‚‹é­”æ³•ã®ã‚ˆã†ãªä»•çµ„ã¿ã€‚\n",
    "\n",
    "- é€šå¸¸: 200å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…¨éƒ¨ã‚’æ›´æ–° â†’ ãƒ¡ãƒ¢ãƒªä¸è¶³\n",
    "- LoRA: 400ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘æ›´æ–° â†’ OKï¼\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“¦ <b>4bité‡å­åŒ–ï¼ˆã‚Šã‚‡ã†ã—ã‹ï¼‰</b></summary>\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’åœ§ç¸®ã™ã‚‹æŠ€è¡“ã€‚  \n",
    "ç”»è³ªã‚’è½ã¨ã—ã¦ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°ã•ãã™ã‚‹ã®ã¨ä¼¼ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "- é€šå¸¸: 40GBå¿…è¦ â†’ ç„¡æ–™GPUã§ã¯å‹•ã‹ãªã„\n",
    "- 4bit: 10GBç¨‹åº¦ â†’ ç„¡æ–™ã®T4 GPUã§å‹•ãï¼\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ¤– <b>GPT-OSS 20B</b></summary>\n",
    "\n",
    "OpenAIãŒå…¬é–‹ã—ãŸ200å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã€‚  \n",
    "ChatGPTã¨ä¼¼ãŸå½¢å¼ã§æ¨è«–ï¼ˆreasoningï¼‰æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ äº‹å‰æº–å‚™ï¼ˆå¿…ãšè¡Œã£ã¦ãã ã•ã„ï¼‰\n",
    "\n",
    "### GPUï¼ˆã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒœãƒ¼ãƒ‰ï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "\n",
    "ã‚³ãƒ¼ãƒ‰ã‚’é«˜é€Ÿã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€GPUã‚’ä½¿ã„ã¾ã™ã€‚  \n",
    "**ç„¡æ–™ã§ä½¿ãˆã‚‹ã®ã§å®‰å¿ƒã—ã¦ãã ã•ã„ï¼**\n",
    "\n",
    "**æ‰‹é †ï¼š**\n",
    "1. ç”»é¢ä¸Šéƒ¨ã®ã€Œ**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. ã€Œ**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´**ã€ã‚’é¸æŠ\n",
    "3. ã€Œ**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿**ã€ã§ã€Œ**T4 GPU**ã€ã‚’é¸æŠ\n",
    "4. ã€Œ**ä¿å­˜**ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "\n",
    "> âš ï¸ **ã“ã®è¨­å®šã‚’ã—ãªã„ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title ğŸ“‹ æº–å‚™ & ãƒ‡ãƒ¼ã‚¿è¨­å®šï¼ˆã“ã®ã‚»ãƒ«ã§è¨­å®šã‚’ç¢ºå®šï¼‰\n# @markdown ---\n# @markdown ### 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯\ngoogle_account = True  # @param {type:\"boolean\"}\ngpu_enabled = False    # @param {type:\"boolean\"}\n\n# @markdown ---\n# @markdown ### 2. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ\ndata_source = \"sample\"  # @param [\"sample\", \"chatgpt\"]\n\n# @markdown ---\n# @markdown ### 3. ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ä½¿ã†å ´åˆã¯ã“ã“ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\nupload_now = False  # @param {type:\"boolean\"}\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# å‡¦ç†\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n# é¸æŠè‚¢ã®èª¬æ˜\nOPTIONS = {\n    \"sample\": \"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ¢ç”¨ãƒ»HuggingFaceï¼‰\",\n    \"chatgpt\": \"ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\"\n}\nuse_chatgpt_export = (data_source == \"chatgpt\")\n\nprint(\"=\" * 50)\nprint(\"ğŸ“‹ è¨­å®šå†…å®¹\")\nprint(\"=\" * 50)\n\n# ç’°å¢ƒãƒã‚§ãƒƒã‚¯\nmissing = []\nif not google_account: missing.append(\"Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆ\")\nif not gpu_enabled: missing.append(\"GPUè¨­å®š\")\n\nif missing:\n    print(f\"âš ï¸ ç’°å¢ƒ: {', '.join(missing)} ãŒæœªç¢ºèª\")\nelse:\n    print(\"âœ… ç’°å¢ƒ: OK\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹\nprint(f\"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿: {OPTIONS[data_source]}\")\n\n# ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\nif use_chatgpt_export and upload_now:\n    print(\"\\nğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:\")\n    from google.colab import files\n    uploaded = files.upload()\n    if 'conversations.json' in uploaded:\n        print(\"âœ… conversations.json ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸï¼\")\n    else:\n        print(\"âš ï¸ conversations.json ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\nelif use_chatgpt_export:\n    print(\"\\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã€Œupload_nowã€ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦å†å®Ÿè¡Œã™ã‚‹ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"âœ… è¨­å®šå®Œäº†ï¼æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 1/7 - ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| **1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | â³ ã¾ã  |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | â³ ã¾ã  |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | â³ ã¾ã  |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | â³ ã¾ã  |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | â³ ã¾ã  |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚  \n",
    "**æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ãŒã€è‡ªå‹•ã§é€²ã‚€ã®ã§å¾…ã£ã¦ã„ã¦ãã ã•ã„ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# ã“ã®ã‚»ãƒ«ã¯è‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚å®Œäº†ã¾ã§æ•°åˆ†ãŠå¾…ã¡ãã ã•ã„ã€‚\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã®ç¢ºèª\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 2/7 - ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| **2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | â³ ã¾ã  |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | â³ ã¾ã  |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | â³ ã¾ã  |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | â³ ã¾ã  |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "GPT-OSS 20Bãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚  \n",
    "**4bité‡å­åŒ–ã‚’ä½¿ã†ã®ã§ã€ç„¡æ–™ã®T4 GPUã§ã‚‚å‹•ãã¾ã™ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ¤– GPT-OSS 20B ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ğŸ“Š è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "max_seq_length = 1024  # ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆé•·ã„ä¼šè©±ã¯åˆ†å‰²ã•ã‚Œã¾ã™ï¼‰\n",
    "dtype = None           # è‡ªå‹•æ¤œå‡ºï¼ˆå¤‰æ›´ä¸è¦ï¼‰\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "print(\"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "    dtype = dtype,                        # è‡ªå‹•æ¤œå‡º\n",
    "    max_seq_length = max_seq_length,      # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·\n",
    "    load_in_4bit = True,                  # 4bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„\n",
    "    full_finetuning = False,              # LoRAã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„ï¼‰\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®š\n",
    "\n",
    "LoRAï¼ˆLow-Rank Adaptationï¼‰ã‚’ä½¿ã†ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã® **ã‚ãšã‹0.02%** ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘ã‚’å­¦ç¿’ã—ã¾ã™ã€‚  \n",
    "ã“ã‚Œã«ã‚ˆã‚Šã€å°‘ãªã„ãƒ¡ãƒ¢ãƒªã¨çŸ­ã„æ™‚é–“ã§å­¦ç¿’ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ§© LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®š\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,  # LoRAã®ãƒ©ãƒ³ã‚¯ï¼ˆ8, 16, 32, 64, 128 ã‹ã‚‰é¸æŠã€‚å¤§ãã„ã»ã©è¡¨ç¾åŠ›ãŒå‘ä¸Šã—ã¾ã™ãŒã€GPUãƒ¡ãƒ¢ãƒªã‚‚æ¶ˆè²»ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ï¼‰\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,  # 0ãŒæœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹\n",
    "    bias = \"none\",     # \"none\"ãŒæœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹\n",
    "    use_gradient_checkpointing = \"unsloth\",  # ãƒ¡ãƒ¢ãƒªç¯€ç´„\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 3/7 - ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | âœ… å®Œäº† |\n",
    "| **3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾—** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | â³ ã¾ã  |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | â³ ã¾ã  |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | â³ ã¾ã  |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "### ğŸ“¥ ChatGPTã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹æ–¹æ³•\n",
    "\n",
    "#### ã‚¹ãƒ†ãƒƒãƒ—1: ChatGPTã®è¨­å®šã‚’é–‹ã\n",
    "1. [ChatGPT](https://chat.openai.com) ã«ãƒ­ã‚°ã‚¤ãƒ³\n",
    "2. å·¦ä¸‹ã® **è‡ªåˆ†ã®åå‰** ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "3. ã€Œ**Settings**ã€ã‚’é¸æŠ\n",
    "\n",
    "#### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
    "1. ã€Œ**Data controls**ã€ã‚¿ãƒ–ã‚’é¸æŠ\n",
    "2. ã€Œ**Export data**ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "3. ãƒ¡ãƒ¼ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãŒå±Šãã¾ã™ï¼ˆæ•°åˆ†ã€œæ•°æ™‚é–“ï¼‰\n",
    "\n",
    "#### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
    "- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPã‚’è§£å‡\n",
    "- `conversations.json` ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ãªã‚Šã¾ã™\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **ã¾ã ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¦ã„ãªã„å ´åˆ**  \n",
    "> ä¸‹ã®ã‚»ãƒ«ã§ã€Œã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã€ã‚’é¸æŠã™ã‚‹ã¨ã€ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã‚’è©¦ã›ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¢ºèª\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n# æœ€åˆã®ã‚»ãƒ«(cell-4)ã§è¨­å®šã—ãŸå†…å®¹ã‚’ç¢ºèª\nprint(f\"ğŸ“‚ é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {OPTIONS[data_source]}\")\n\nif use_chatgpt_export:\n    print(\"\\nğŸ“¤ ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™\")\n    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒã¾ã ã®å ´åˆ\n    import os\n    if not os.path.exists(\"conversations.json\"):\n        print(\"âš ï¸ conversations.json ãŒã¾ã ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n        print(\"   æœ€åˆã®ã‚»ãƒ«ã§ã€Œupload_nowã€ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n    else:\n        print(\"âœ… conversations.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\")\nelse:\n    print(\"\\nğŸ“š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ğŸ“¤ è¿½åŠ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nimport os\n\nif use_chatgpt_export and not os.path.exists(\"conversations.json\"):\n    print(\"ğŸ“ conversations.json ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n    from google.colab import files\n    uploaded = files.upload()\n    \n    if 'conversations.json' in uploaded:\n        print(\"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n    else:\n        print(\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n        use_chatgpt_export = False\nelse:\n    print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„ã€‚\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 4/7 - ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | âœ… å®Œäº† |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | âœ… å®Œäº† |\n",
    "| **4. ãƒ‡ãƒ¼ã‚¿æº–å‚™** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | â³ ã¾ã  |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | â³ ã¾ã  |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã®å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ”„ ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®å¤‰æ›é–¢æ•°\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "import json\n",
    "\n",
    "def load_chatgpt_conversations(file_path=\"conversations.json\"):\n",
    "    \"\"\"\n",
    "    ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›\n",
    "\n",
    "    å¼•æ•°:\n",
    "        file_path: conversations.jsonã®ãƒ‘ã‚¹\n",
    "\n",
    "    æˆ»ã‚Šå€¤:\n",
    "        å­¦ç¿’ç”¨ã®ä¼šè©±ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    for conversation in data:\n",
    "        messages = []\n",
    "        mapping = conversation.get('mapping', {})\n",
    "\n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é †ç•ªã«å–å¾—\n",
    "        for node_id, node in mapping.items():\n",
    "            msg = node.get('message')\n",
    "            if msg and msg.get('content', {}).get('parts'):\n",
    "                role = msg['author']['role']\n",
    "                parts = msg['content']['parts']\n",
    "                if parts and isinstance(parts[0], str):\n",
    "                    content = parts[0].strip()\n",
    "\n",
    "                    # userã¨assistantã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿æŠ½å‡º\n",
    "                    if role in ['user', 'assistant'] and content:\n",
    "                        messages.append({\n",
    "                            \"role\": role,\n",
    "                            \"content\": content\n",
    "                        })\n",
    "\n",
    "        # æœ‰åŠ¹ãªä¼šè©±ã®ã¿è¿½åŠ ï¼ˆæœ€ä½2ã‚¿ãƒ¼ãƒ³ï¼‰\n",
    "        if len(messages) >= 2:\n",
    "            training_data.append({\"messages\": messages})\n",
    "\n",
    "    print(f\"âœ… {len(training_data)} ä»¶ã®ä¼šè©±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def filter_conversations(data, min_turns=2, max_turns=20, keywords=None):\n",
    "    \"\"\"\n",
    "    ä¼šè©±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "\n",
    "    - min_turns: æœ€å°ã‚¿ãƒ¼ãƒ³æ•°\n",
    "    - max_turns: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆé•·ã™ãã‚‹ä¼šè©±ã‚’é™¤å¤–ï¼‰\n",
    "    - keywords: å«ã‚ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for conv in data:\n",
    "        turns = len(conv['messages'])\n",
    "        if min_turns <= turns <= max_turns:\n",
    "            if keywords is None:\n",
    "                filtered.append(conv)\n",
    "            else:\n",
    "                text = ' '.join([m['content'] for m in conv['messages']])\n",
    "                if any(kw in text for kw in keywords):\n",
    "                    filtered.append(conv)\n",
    "\n",
    "    print(f\"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered)} ä»¶\")\n",
    "    return filtered\n",
    "\n",
    "print(\"âœ… å¤‰æ›é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"ä¼šè©±ã‚’GPT-OSSå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›\"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "if use_chatgpt_export:\n",
    "    # ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ä½¿ç”¨\n",
    "    print(\"ğŸ”„ ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’å¤‰æ›ä¸­...\")\n",
    "    raw_data = load_chatgpt_conversations(\"conversations.json\")\n",
    "    \n",
    "    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    filtered_data = filter_conversations(raw_data, min_turns=2, max_turns=20)\n",
    "    \n",
    "    # Datasetã«å¤‰æ›\n",
    "    dataset = Dataset.from_list(filtered_data)\n",
    "else:\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "    print(\"ğŸ”„ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "    \n",
    "    # ShareGPTå½¢å¼ã«æ¨™æº–åŒ–\n",
    "    from unsloth.chat_templates import standardize_sharegpt\n",
    "    dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:\")\n",
    "print(f\"   - ä¼šè©±æ•°: {len(dataset)} ä»¶\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ğŸ‘€ ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèªï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰\n",
    "# @markdown å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚ˆã†ãªå½¢å¼ã‹ç¢ºèªã§ãã¾ã™\n",
    "\n",
    "show_sample = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if show_sample and len(dataset) > 0:\n",
    "    print(\"ğŸ“ 1ä»¶ç›®ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸€éƒ¨ï¼‰:\")\n",
    "    print(\"=\" * 60)\n",
    "    sample_text = dataset[0]['text']\n",
    "    # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚\n",
    "    if len(sample_text) > 1500:\n",
    "        print(sample_text[:1500] + \"\\n\\n... (ä»¥ä¸‹çœç•¥)\")\n",
    "    else:\n",
    "        print(sample_text)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 5/7 - å­¦ç¿’å®Ÿè¡Œ\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | âœ… å®Œäº† |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | âœ… å®Œäº† |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | âœ… å®Œäº† |\n",
    "| **5. å­¦ç¿’å®Ÿè¡Œ** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | â³ ã¾ã  |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "ã„ã‚ˆã„ã‚ˆå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title âš™ï¸ å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "# @markdown å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„\n",
    "\n",
    "# ğŸ“Š å­¦ç¿’è¨­å®šã®èª¿æ•´ã‚¬ã‚¤ãƒ‰\n",
    "#\n",
    "# max_steps: å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "#   - 30ï¼ˆãƒ†ã‚¹ãƒˆç”¨ãƒ»ç´„10åˆ†ï¼‰\n",
    "#   - 100ï¼ˆè»½ã„å­¦ç¿’ãƒ»ç´„30åˆ†ï¼‰\n",
    "#   - 500ï¼ˆã—ã£ã‹ã‚Šå­¦ç¿’ãƒ»ç´„2æ™‚é–“ï¼‰\n",
    "#\n",
    "# learning_rate: å­¦ç¿’ç‡\n",
    "#   - 2e-4ï¼ˆæ¨å¥¨ï¼šå®‰å®šã—ãŸå­¦ç¿’ï¼‰\n",
    "#   - 1e-4ï¼ˆã‚ˆã‚Šæ…é‡ãªå­¦ç¿’ï¼‰\n",
    "\n",
    "max_steps = 30  # @param {type:\"slider\", min:10, max:500, step:10}\n",
    "learning_rate = 2e-4  # @param {type:\"number\"}\n",
    "batch_size = 1  # @param {type:\"slider\", min:1, max:4, step:1}\n",
    "\n",
    "print(f\"ğŸ“Š å­¦ç¿’è¨­å®š:\")\n",
    "print(f\"   - å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: {max_steps}\")\n",
    "print(f\"   - å­¦ç¿’ç‡: {learning_rate}\")\n",
    "print(f\"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ“ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = max_steps,\n",
    "        learning_rate = learning_rate,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸ¯ å›ç­”éƒ¨åˆ†ã®ã¿ã‚’å­¦ç¿’å¯¾è±¡ã«ã™ã‚‹\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã¯å­¦ç¿’ã›ãšã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ã®ã¿ã‚’å­¦ç¿’ã—ã¾ã™\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„å­¦ç¿’ãŒã§ãã¾ã™\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "gpt_oss_kwargs = dict(\n",
    "    instruction_part = \"<|start|>user<|message|>\",\n",
    "    response_part = \"<|start|>assistant<|channel|>final<|message|>\"\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    **gpt_oss_kwargs,\n",
    ")\n",
    "\n",
    "print(\"âœ… å›ç­”éƒ¨åˆ†ã®ã¿ã‚’å­¦ç¿’å¯¾è±¡ã«è¨­å®šã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ğŸ’¾ ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"ğŸ–¥ï¸ GPU: {gpu_stats.name}\")\n",
    "print(f\"ğŸ’¾ æœ€å¤§ãƒ¡ãƒ¢ãƒª: {max_memory} GB\")\n",
    "print(f\"ğŸ“Š ç¾åœ¨ä½¿ç”¨ä¸­: {start_gpu_memory} GB ({round(start_gpu_memory/max_memory*100, 1)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# ğŸš€ å­¦ç¿’é–‹å§‹ï¼\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "print(\"ğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "print(f\"   è¨­å®š: {max_steps} ã‚¹ãƒ†ãƒƒãƒ—\")\n",
    "print(\"   â”\" * 30)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"â”\" * 60)\n",
    "print(\"ğŸ‰ å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ğŸ“Š å­¦ç¿’çµæœã®ã‚µãƒãƒªãƒ¼\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "print(\"ğŸ“Š å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"â±ï¸ å­¦ç¿’æ™‚é–“: {round(training_time, 1)} ç§’ ({round(training_time/60, 1)} åˆ†)\")\n",
    "print(f\"ğŸ’¾ ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {used_memory} GB\")\n",
    "print(f\"ğŸ“ˆ å­¦ç¿’ç”¨è¿½åŠ ãƒ¡ãƒ¢ãƒª: {used_memory_for_lora} GB\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 6/7 - ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | âœ… å®Œäº† |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | âœ… å®Œäº† |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | âœ… å®Œäº† |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | âœ… å®Œäº† |\n",
    "| **6. ãƒ†ã‚¹ãƒˆ** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "| 7. ä¿å­˜ | â³ ã¾ã  |\n",
    "\n",
    "å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title ğŸ’¬ ãƒ¢ãƒ‡ãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹\n# @markdown å¥½ããªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’ç¢ºèªã§ãã¾ã™\n\nuser_message = \"Hello! Please introduce yourself.\"  # @param {type:\"string\"}\nreasoning_effort = \"medium\"  # @param [\"low\", \"medium\", \"high\"]\n\n# æ—¥æœ¬èªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›ï¼ˆç©ºã®å ´åˆï¼‰\nif user_message == \"Hello! Please introduce yourself.\":\n    user_message = \"ã“ã‚“ã«ã¡ã¯ï¼è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": user_message},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n    return_dict = True,\n    reasoning_effort = reasoning_effort,\n).to(\"cuda\")\n\n# æ¨è«–ãƒ¬ãƒ™ãƒ«ã®èª¬æ˜\nREASONING_DESC = {\"low\": \"ä½ï¼ˆé«˜é€Ÿï¼‰\", \"medium\": \"ä¸­ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰\", \"high\": \"é«˜ï¼ˆè©³ç´°ï¼‰\"}\n\nprint(f\"ğŸ’¬ ã‚ãªãŸ: {user_message}\")\nprint(f\"ğŸ§  æ¨è«–ãƒ¬ãƒ™ãƒ«: {REASONING_DESC[reasoning_effort]}\")\nprint(\"\\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®å›ç­”:\")\nprint(\"-\" * 40)\n\nfrom transformers import TextStreamer\n_ = model.generate(**inputs, max_new_tokens=256, streamer=TextStreamer(tokenizer))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ æ¨è«–ãƒ¬ãƒ™ãƒ«ï¼ˆReasoning Effortï¼‰ã«ã¤ã„ã¦\n",
    "\n",
    "GPT-OSSã«ã¯3ã¤ã®æ¨è«–ãƒ¬ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "\n",
    "| ãƒ¬ãƒ™ãƒ« | ç‰¹å¾´ | é©ã—ãŸç”¨é€” |\n",
    "|--------|------|------------|\n",
    "| **low** | é«˜é€Ÿã€ã‚·ãƒ³ãƒ—ãƒ«ãªå›ç­” | ç°¡å˜ãªè³ªå•ã€ãƒãƒ£ãƒƒãƒˆ |\n",
    "| **medium** | ãƒãƒ©ãƒ³ã‚¹å‹ | ä¸€èˆ¬çš„ãªè³ªå• |\n",
    "| **high** | æ·±ã„æ€è€ƒã€è©³ç´°ãªå›ç­” | è¤‡é›‘ãªå•é¡Œã€åˆ†æ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 7/7 - ä¿å­˜\n",
    "\n",
    "| ã‚¹ãƒ†ãƒƒãƒ— | çŠ¶æ…‹ |\n",
    "|---------|------|\n",
    "| 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | âœ… å®Œäº† |\n",
    "| 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ | âœ… å®Œäº† |\n",
    "| 3. ChatGPTãƒ‡ãƒ¼ã‚¿å–å¾— | âœ… å®Œäº† |\n",
    "| 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™ | âœ… å®Œäº† |\n",
    "| 5. å­¦ç¿’å®Ÿè¡Œ | âœ… å®Œäº† |\n",
    "| 6. ãƒ†ã‚¹ãƒˆ | âœ… å®Œäº† |\n",
    "| **7. ä¿å­˜** | ğŸ”„ **ã„ã¾ã“ã“** |\n",
    "\n",
    "å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ã‚‡ã†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title ğŸ’¾ ä¿å­˜æ–¹æ³•ã‚’é¸æŠ\n# @markdown ã©ã“ã«ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ\n\nsave_location = \"colab\"  # @param [\"colab\", \"gdrive\", \"huggingface\"]\n\n# é¸æŠè‚¢ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰\nSAVE_OPTIONS = {\n    \"colab\": \"Colabã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸€æ™‚çš„ï¼‰\",\n    \"gdrive\": \"Google Driveï¼ˆæ°¸ç¶šä¿å­˜ï¼‰\",\n    \"huggingface\": \"Hugging Face Hubï¼ˆå…¬é–‹/å…±æœ‰ï¼‰\"\n}\n\nprint(f\"ğŸ“ ä¿å­˜å…ˆ: {SAVE_OPTIONS[save_location]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nmodel_name = \"my_gpt_oss_20b\"\n\nif save_location == \"colab\":\n    # Colabã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã§æ¶ˆãˆã¾ã™ï¼‰\n    model.save_pretrained(model_name)\n    print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_name}' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n    print(\"âš ï¸ æ³¨æ„: Colabã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒçµ‚äº†ã™ã‚‹ã¨æ¶ˆãˆã¾ã™\")\n\nelif save_location == \"gdrive\":\n    # Google Driveã«ä¿å­˜ï¼ˆæ°¸ç¶šçš„ï¼‰\n    from google.colab import drive\n    print(\"ğŸ”— Google Driveã«æ¥ç¶šä¸­...\")\n    drive.mount('/content/drive')\n    \n    save_path = f\"/content/drive/MyDrive/{model_name}\"\n    model.save_pretrained(save_path)\n    print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ Google Drive ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n    print(f\"   ãƒ‘ã‚¹: {save_path}\")\n\nelif save_location == \"huggingface\":\n    # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n    print(\"ğŸ“¤ Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯:\")\n    print(\"   1. https://huggingface.co/settings/tokens ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\")\n    print(\"   2. ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤ã—ã¦å®Ÿè¡Œ\")\n    print(\"\")\n    print('# model.push_to_hub(\"ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å/my_gpt_oss_20b\", token=\"hf_...\")')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ å®Œäº†ï¼\n",
    "\n",
    "ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ã‚ãªãŸå°‚ç”¨ã®GPT-OSS 20Bãƒ¢ãƒ‡ãƒ«ãŒå®Œæˆã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ğŸ”„ ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•\n",
    "\n",
    "æ¬¡å›ã€ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãã ã•ã„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ä¾‹ï¼ˆå¿…è¦ãªæ™‚ã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ï¼‰\n",
    "\n",
    "if False:  # True ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"my_gpt_oss_20b\",  # ä¿å­˜æ™‚ã®åå‰\n",
    "        max_seq_length = 1024,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ†˜ å›°ã£ãŸã¨ãã¯\n",
    "\n",
    "### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ³•\n",
    "\n",
    "<details>\n",
    "<summary>âŒ <b>CUDA out of memory</b></summary>\n",
    "\n",
    "**åŸå› **: GPUã®ãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šã¾ã›ã‚“\n",
    "\n",
    "**è§£æ±ºæ³•**:\n",
    "1. ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã€\n",
    "2. `max_seq_length` ã‚’ 512 ã«ä¸‹ã’ã‚‹\n",
    "3. `batch_size` ã‚’ 1 ã«ã™ã‚‹\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âŒ <b>No GPU runtime</b></summary>\n",
    "\n",
    "**åŸå› **: GPUãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã›ã‚“\n",
    "\n",
    "**è§£æ±ºæ³•**:\n",
    "ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ã€â†’ã€ŒT4 GPUã€ã‚’é¸æŠ\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âŒ <b>conversations.json ãŒèª­ã¿è¾¼ã‚ãªã„</b></summary>\n",
    "\n",
    "**åŸå› **: ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å•é¡Œ\n",
    "\n",
    "**è§£æ±ºæ³•**:\n",
    "1. ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "2. ZIPã‚’è§£å‡ã—ã¦ã‹ã‚‰ .json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "3. ãƒ•ã‚¡ã‚¤ãƒ«åãŒ `conversations.json` ã‹ç¢ºèª\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âŒ <b>å­¦ç¿’ãŒé…ã„ãƒ»å›ºã¾ã‚‹</b></summary>\n",
    "\n",
    "**åŸå› **: ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹ã€ã¾ãŸã¯GPUã®å•é¡Œ\n",
    "\n",
    "**è§£æ±ºæ³•**:\n",
    "1. `max_steps` ã‚’æ¸›ã‚‰ã™ï¼ˆ30ç¨‹åº¦ã§ãƒ†ã‚¹ãƒˆï¼‰\n",
    "2. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ä»¶æ•°ã‚’æ¸›ã‚‰ã™\n",
    "3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦å†å®Ÿè¡Œ\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ ã‚µãƒãƒ¼ãƒˆ\n",
    "\n",
    "- Unsloth Discord: https://discord.gg/unsloth\n",
    "- Unsloth ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://docs.unsloth.ai/\n",
    "- GitHub: https://github.com/unslothai/unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "å­¦ç¿’ã‚’çµ‚ãˆãŸã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç™ºå±•çš„ãªä½¿ã„æ–¹ã‚‚ã§ãã¾ã™ï¼š\n",
    "\n",
    "### ğŸ“š ã‚‚ã£ã¨å­¦ã¶\n",
    "\n",
    "1. **å¼·åŒ–å­¦ç¿’ï¼ˆGRPOï¼‰ã§ã•ã‚‰ã«è³¢ã** - [Llama GRPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. **Ollamaã§ç°¡å˜ã«ä½¿ã†** - [Ollamaä¿å­˜ notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. **ç”»åƒã‚‚ç†è§£ã™ã‚‹ãƒ¢ãƒ‡ãƒ«** - [Vision Fine-tuning](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "\n",
    "### ğŸ”— ãƒªãƒ³ã‚¯\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme) ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}